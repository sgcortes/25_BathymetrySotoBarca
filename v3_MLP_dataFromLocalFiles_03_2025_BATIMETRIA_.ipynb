{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOLOry5CAY1TOAPZNNdKnJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgcortes/25_BathymetrySotoBarca/blob/main/v3_MLP_dataFromLocalFiles_03_2025_BATIMETRIA_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimaci√≥n de la Batimetr√≠a a partir de imagen UAV multiespectral. MLP (Multilayer Perceptron)\n",
        "+Cota l√°mina de agua 212m\n",
        "\n",
        "---\n",
        "En la hoja excel adjunta (NorteEsteProf.xlsx) y en la en el hoja OesteProf.xlsx, se tienen datos de cotas del fondo (\"z\") de un embalse para puntos de posiciones (campos \"x\" e \"y\"). Adem√°s hay un conjunto de valores de niveles de reflectancia (campos: \"red\", \"green\", \"blue\", \"red_edge\", \"nir\") y de otros √≠ndices multiespectrales (\"ratiobg\", \"ndwi\", \"kwlightate\",\"ndvibarca\") que han sido calculados a partir de combinaciones de las variables de reflectancia.\n",
        "\n",
        "El objetivo es entrenar un modelo GradientBoost (sklearn) que relacione estas variables (excepto, \"fid\", \"x\", \"y\", \"z\") con la \"Profundidad\" (calculada como : 212-\"z\"). Los datos suministrados (alrededor de 16000 mas 3000) se dividir√°n en 60% para entrenamiento y 20% para test y 20% para validaci√≥n.\n",
        "al final del entrenamiento dame las m√©tricas del resultado obtenido para el conjunto test y el del entrenamiento y valiadci√≥n por separado. Adem√°s crea tres gr√°ficas de valores predichos frente a valores reales para la profundad para el conjunto test y para el de entrenamiento y validaci√≥n. Dibuja tambi√©n una gr√°fica de importancia de ls variables en barras horizontales y ordeandas de mayor a menor."
      ],
      "metadata": {
        "id": "E9k20MB1YY28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura de datos"
      ],
      "metadata": {
        "id": "BG1MnbqlvPEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Optuna si no est√° instalado (ya que el entorno fue reiniciado)\n",
        "!pip install optuna\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP8GJnzcX8vv",
        "outputId": "c918d9f4-0040-41b0-9aa3-c26528bdad46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Iniciar temporizador\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "liSIznp92P8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ACMnYrDEp-g9",
        "outputId": "c2e0c994-8ed0-4170-a958-9570cb805235"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'NorteEsteProf.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-042767a3345c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfile_oeste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"OesteProf.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_norte_este\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_norte_este\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf_oeste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_oeste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NorteEsteProf.xlsx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "# Montar Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta del directorio en Google Drive\n",
        "#path = \"/content/drive/My Drive/Colab Notebooks/03_2025_BATIMETRIA-SOTO-BARCA/\"\n",
        "\n",
        "# Leer los archivos de Excel\n",
        "file_norte_este = \"NorteEsteProf.xlsx\"\n",
        "file_oeste = \"OesteProf.xlsx\"\n",
        "\n",
        "df_norte_este = pd.read_excel(file_norte_este)\n",
        "df_oeste = pd.read_excel(file_oeste)\n",
        "\n",
        "# Mostrar las primeras filas de los DataFrames para verificar la carga\n",
        "df_norte_este.head(), df_oeste.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recargar librer√≠as necesarias despu√©s del reinicio del entorno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Definir las caracter√≠sticas y la variable objetivo\n",
        "features = [\"red\", \"green\", \"blue\", \"red_edge\", \"nir\", \"ratiobg\", \"ndwi\", \"kwlightate\", \"ndvibarca\"]\n",
        "target = \"Profundidad\"\n",
        "\n",
        "# Unir ambos conjuntos de datos en un solo DataFrame\n",
        "df_total = pd.concat([df_norte_este, df_oeste], ignore_index=True)\n",
        "\n",
        "# Imputar valores nulos con la mediana de cada columna\n",
        "df_total.fillna(df_total.median(), inplace=True)\n",
        "\n",
        "# Separar en variables independientes (X) y la variable objetivo (y)\n",
        "X = df_total[features]\n",
        "y = df_total[target]\n",
        "\n",
        "# Dividir en conjunto de entrenamiento (60%), test (20%) y validaci√≥n (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Normalizar las caracter√≠sticas para mejorar la estabilidad del entrenamiento\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Definir la arquitectura de la red neuronal\n",
        "mlp_model = MLPRegressor(hidden_layer_sizes=(128, 64, 32),\n",
        "                         activation='relu',\n",
        "                         solver='adam',\n",
        "                         max_iter=1000,\n",
        "                         random_state=42)\n",
        "\n",
        "# Entrenar la red neuronal\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones con la red entrenada\n",
        "y_train_pred_mlp = mlp_model.predict(X_train_scaled)\n",
        "y_test_pred_mlp = mlp_model.predict(X_test_scaled)\n",
        "y_val_pred_mlp = mlp_model.predict(X_val_scaled)\n",
        "\n",
        "# Calcular m√©tricas para el modelo MLP\n",
        "metrics_train_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_train, y_train_pred_mlp),\n",
        "    \"MSE\": mean_squared_error(y_train, y_train_pred_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_train, y_train_pred_mlp)),\n",
        "    \"R¬≤\": r2_score(y_train, y_train_pred_mlp)\n",
        "}\n",
        "\n",
        "metrics_test_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_test, y_test_pred_mlp),\n",
        "    \"MSE\": mean_squared_error(y_test, y_test_pred_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_test_pred_mlp)),\n",
        "    \"R¬≤\": r2_score(y_test, y_test_pred_mlp)\n",
        "}\n",
        "\n",
        "metrics_val_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_val, y_val_pred_mlp),\n",
        "    \"MSE\": mean_squared_error(y_val, y_val_pred_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_val, y_val_pred_mlp)),\n",
        "    \"R¬≤\": r2_score(y_val, y_val_pred_mlp)\n",
        "}\n"
      ],
      "metadata": {
        "id": "NiNnWryM8zUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar m√©tricas\n",
        "print(\"\\nTrain:\\n\",metrics_train_mlp, \"\\nTest:\\n\",metrics_test_mlp,\"\\nValidation:\\n\", metrics_val_mlp)\n"
      ],
      "metadata": {
        "id": "pQCwK6jO9l7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An√°lisis:\n",
        "El modelo tiene un R¬≤ ‚âà 0.74 en el conjunto de test, lo que indica que la red neuronal logra explicar una buena parte de la variabilidad de la profundidad.\n",
        "El rendimiento en test y validaci√≥n es similar al de entrenamiento, lo que indica que el modelo no est√° sobreajustado.\n",
        "Sin embargo, en comparaci√≥n con el Gradient Boosting, el desempe√±o parece un poco inferior."
      ],
      "metadata": {
        "id": "JY9_NfcJvTK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gr√°ficos predichos vs Reales"
      ],
      "metadata": {
        "id": "9YwDJloz9333"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear gr√°ficos de valores predichos vs. valores reales\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Gr√°fico para el conjunto de entrenamiento\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_train, y_train_pred_mlp, alpha=0.5)\n",
        "plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Profundidad Real\")\n",
        "plt.ylabel(\"Profundidad Predicha\")\n",
        "plt.title(\"Entrenamiento: Valores Reales vs. Predichos\")\n",
        "\n",
        "# Gr√°fico para el conjunto de test\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(y_test, y_test_pred_mlp, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Profundidad Real\")\n",
        "plt.ylabel(\"Profundidad Predicha\")\n",
        "plt.title(\"Test: Valores Reales vs. Predichos\")\n",
        "\n",
        "# Gr√°fico para el conjunto de validaci√≥n\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(y_val, y_val_pred_mlp, alpha=0.5)\n",
        "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Profundidad Real\")\n",
        "plt.ylabel(\"Profundidad Predicha\")\n",
        "plt.title(\"Validaci√≥n: Valores Reales vs. Predichos\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5LWHo7qfrgJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An√°lisis de los gr√°ficos:\n",
        "\n",
        "En entrenamiento, los valores predichos siguen bastante bien la l√≠nea de identidad roja, lo que indica un buen ajuste.\n",
        "En test y validaci√≥n, se mantiene una tendencia similar, aunque con mayor dispersi√≥n, lo que sugiere que la red neuronal generaliza razonablemente bien.\n"
      ],
      "metadata": {
        "id": "-_eeQ554-Agx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importancia de las variables"
      ],
      "metadata": {
        "id": "EZD77v6-vvmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Calcular la importancia de las variables usando Permutation Feature Importance\n",
        "perm_importance = permutation_importance(mlp_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Convertir los resultados a un DataFrame\n",
        "importances_df = pd.DataFrame({'Variable': features, 'Importancia': perm_importance.importances_mean})\n",
        "importances_df = importances_df.sort_values(by='Importancia', ascending=True)\n",
        "\n",
        "# Graficar la importancia de las variables\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importances_df['Variable'], importances_df['Importancia'], color='skyblue')\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.ylabel(\"Variables\")\n",
        "plt.title(\"Importancia de las Variables en el Modelo MLP (Permutation Importance)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r3Sjn6ETv07G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Training:\\n\",metrics_train_mlp, \"\\nTest:\\n\",metrics_test_mlp,\"\\nValidation:\\n\", metrics_val_mlp)\n"
      ],
      "metadata": {
        "id": "4grzz-I6wZOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An√°lisis:\n",
        "\n",
        "Algunas variables tienen una mayor influencia en la predicci√≥n de la profundidad del embalse.\n",
        "Al no haber una estructura clara como en los modelos de √°rboles, la influencia de cada variable puede ser m√°s distribuida y dependiente de interacciones no lineales.\n",
        "Las variables con mayor importancia indican que tienen una mayor contribuci√≥n a la precisi√≥n del modelo."
      ],
      "metadata": {
        "id": "XZYBdGLawjJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardado del mejor modelo"
      ],
      "metadata": {
        "id": "pNLsUpIpyZbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(mlp_model, \"mlp_regressor_model.pkl\")\n",
        "print(\"Modelo MLP guardado como 'mlp_regressor_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "Rqn6AGNjycXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga del mejor modelo y prediccion"
      ],
      "metadata": {
        "id": "Kd1xVMvpyhH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo desde el archivo\n",
        "loaded_mlp_model = joblib.load(\"mlp_regressor_model.pkl\")\n",
        "\n",
        "# Usar el modelo cargado para hacer predicciones\n",
        "y_pred_loaded = loaded_mlp_model.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar las primeras 5 predicciones\n",
        "print(\"Predicciones con el modelo cargado:\", y_pred_loaded[:5])"
      ],
      "metadata": {
        "id": "BWmPBrhtymVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conjunto de datos"
      ],
      "metadata": {
        "id": "ECgZ-eTT3-Yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El grupo de datos completamente independiente, que no ha intervenido en ning√∫n momento en el entrenamiento ni en la optimizaci√≥n de hiperpar√°metros, es el conjunto de **test**.\n",
        "\n",
        "**Entrenamiento (60%):**\n",
        "\n",
        "Se us√≥ para ajustar el modelo tanto en la versi√≥n inicial como en la versi√≥n optimizada con Optuna.\n",
        "Particip√≥ en la estimaci√≥n de los mejores valores de los hiperpar√°metros.\n",
        "\n",
        "**Validaci√≥n (20%):**\n",
        "\n",
        "Se us√≥ en Optuna para evaluar los diferentes modelos mientras se ajustaban los hiperpar√°metros.\n",
        "Se utiliz√≥ en el c√°lculo del RMSE para determinar el mejor conjunto de hiperpar√°metros.\n",
        "\n",
        "**Test (20%):**\n",
        "\n",
        "No se us√≥ en ning√∫n momento durante el entrenamiento ni en la optimizaci√≥n de hiperpar√°metros.\n",
        "Se utiliz√≥ √∫nicamente despu√©s de obtener el mejor modelo para evaluar su capacidad de generalizaci√≥n en datos completamente nuevos."
      ],
      "metadata": {
        "id": "gVFNgy5O6x7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MEJORA DE LA RED NEURONAL\n",
        "### Optimizaci√≥n con Optuna\n",
        "¬øQu√© vamos a optimizar?\n",
        "1. N√∫mero de capas ocultas (entre 1 y 4 capas).\n",
        "2. N√∫mero de neuronas por capa (entre 32 y 512).\n",
        "3. Tasa de regularizaci√≥n alpha (para evitar sobreajuste).\n",
        "4. Tipo de optimizador (adam vs sgd).\n",
        "5. N√∫mero de iteraciones (max_iter)."
      ],
      "metadata": {
        "id": "fn92x-qh_WL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar Optuna\n",
        "import optuna\n",
        "import optuna.visualization as vis\n",
        "# Definir la funci√≥n de optimizaci√≥n con Optuna\n",
        "def objective(trial):\n",
        "    # Hiperpar√°metros a optimizar\n",
        "    hidden_layer_1 = trial.suggest_int(\"hidden_layer_1\", 32, 512)\n",
        "    hidden_layer_2 = trial.suggest_int(\"hidden_layer_2\", 32, 512)\n",
        "    hidden_layer_3 = trial.suggest_int(\"hidden_layer_3\", 32, 512)\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-1,log=True)  # Regularizaci√≥n L2\n",
        "    solver = trial.suggest_categorical(\"solver\", [\"adam\", \"sgd\"])\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 500, 2000)\n",
        "\n",
        "    # Definir el modelo con los hiperpar√°metros seleccionados\n",
        "    mlp_model = MLPRegressor(hidden_layer_sizes=(hidden_layer_1, hidden_layer_2, hidden_layer_3),\n",
        "                             activation='relu',\n",
        "                             solver=solver,\n",
        "                             alpha=alpha,\n",
        "                             max_iter=max_iter,\n",
        "                             random_state=42)\n",
        "\n",
        "    # Entrenar la red neuronal\n",
        "    mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluar en el conjunto de validaci√≥n\n",
        "    y_val_pred = mlp_model.predict(X_val_scaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))  # Minimizar RMSE\n",
        "\n",
        "    return rmse\n",
        "\n",
        "# Ejecutar la optimizaci√≥n con Optuna\n",
        "study = optuna.create_study(direction=\"minimize\")  # Minimizar el RMSE\n",
        "study.optimize(objective, n_trials=25)  # Ejecutar 30 pruebas de hiperpar√°metros\n",
        "\n",
        "# Obtener los mejores hiperpar√°metros encontrados\n",
        "best_params_mlp = study.best_params\n",
        "\n",
        "# üìä Gr√°ficos de an√°lisis de Optuna\n",
        "# 1Ô∏è‚É£ Optimization History Plot\n",
        "fig1 = vis.plot_optimization_history(study)\n",
        "fig1.show()\n",
        "\n",
        "# 2Ô∏è‚É£ Importancia de los hiperpar√°metros\n",
        "fig2 = vis.plot_param_importances(study)\n",
        "fig2.show()\n",
        "\n",
        "# 3Ô∏è‚É£ Slice Plot (impacto de cada hiperpar√°metro en RMSE)\n",
        "fig3 = vis.plot_slice(study)\n",
        "fig3.show()\n",
        "\n",
        "# 4Ô∏è‚É£ Parallel Coordinate Plot (relaci√≥n entre hiperpar√°metros y RMSE)\n",
        "fig4 = vis.plot_parallel_coordinate(study)\n",
        "fig4.show()"
      ],
      "metadata": {
        "id": "F8_VuQ965HY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el mejor modelo con los hiperpar√°metros √≥ptimos\n",
        "#best_mlp_model = MLPRegressor(**best_params_mlp, activation='relu', random_state=42)\n",
        "hidden_layers = (best_params_mlp[\"hidden_layer_1\"],\n",
        "                 best_params_mlp[\"hidden_layer_2\"],\n",
        "                 best_params_mlp[\"hidden_layer_3\"])\n",
        "\n",
        "# Crear el modelo MLP con los mejores hiperpar√°metros\n",
        "best_mlp_model = MLPRegressor(hidden_layer_sizes=hidden_layers,\n",
        "                              activation='relu',\n",
        "                              solver=best_params_mlp[\"solver\"],\n",
        "                              alpha=best_params_mlp[\"alpha\"],\n",
        "                              max_iter=best_params_mlp[\"max_iter\"],\n",
        "                              random_state=42)\n",
        "\n",
        "best_mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones con el mejor modelo optimizado\n",
        "y_train_pred_best_mlp = best_mlp_model.predict(X_train_scaled)\n",
        "y_test_pred_best_mlp = best_mlp_model.predict(X_test_scaled)\n",
        "y_val_pred_best_mlp = best_mlp_model.predict(X_val_scaled)\n",
        "\n",
        "# Calcular m√©tricas del modelo optimizado\n",
        "metrics_train_best_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_train, y_train_pred_best_mlp),\n",
        "    \"MSE\": mean_squared_error(y_train, y_train_pred_best_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_train, y_train_pred_best_mlp)),\n",
        "    \"R¬≤\": r2_score(y_train, y_train_pred_best_mlp)\n",
        "}\n",
        "\n",
        "metrics_test_best_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_test, y_test_pred_best_mlp),\n",
        "    \"MSE\": mean_squared_error(y_test, y_test_pred_best_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_test_pred_best_mlp)),\n",
        "    \"R¬≤\": r2_score(y_test, y_test_pred_best_mlp)\n",
        "}\n",
        "\n",
        "metrics_val_best_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_val, y_val_pred_best_mlp),\n",
        "    \"MSE\": mean_squared_error(y_val, y_val_pred_best_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_val, y_val_pred_best_mlp)),\n",
        "    \"R¬≤\": r2_score(y_val, y_val_pred_best_mlp)\n",
        "}\n",
        "\n",
        "# Mostrar los mejores hiperpar√°metros y las m√©tricas optimizadas\n",
        "print(\"Best Hyperpars:\\n\",best_params_mlp,\"\\nBest MLP Train:\\n\", metrics_train_best_mlp,\"\\nTest MLP Test:\\n\", metrics_test_best_mlp, \"\\nBest MLP Val:\\n\",metrics_val_best_mlp)\n"
      ],
      "metadata": {
        "id": "dFz8WN-uXzK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En las redes neuronales MLP (Multilayer Perceptron) no hay una m√©trica directa de importancia de las variables.\n",
        "\n",
        "Para estimar la importancia de las variables en un MLP, usamos **Permutation Feature Importance**, que es una t√©cnica basada en el impacto de cada variable en la predicci√≥n.\n",
        "Proceso de c√°lculo:\n",
        "\n",
        "Se calcula el RMSE original del modelo.\n",
        "Se mezcla aleatoriamente una de las variables de entrada.\n",
        "Se vuelve a calcular el RMSE del modelo con la variable alterada.\n",
        "La importancia de la variable se define como la diferencia entre el RMSE original y el RMSE despu√©s de la permutaci√≥n.\n",
        "Cuanto m√°s aumente el error al desordenar una variable, m√°s importante es esa variable.\n",
        "\n",
        "**¬øLas importancias son relativas?**\n",
        " No son relativas ni normalizadas.\n",
        " Son valores absolutos de impacto en el error del modelo.\n",
        "\n",
        "Si una variable tiene una importancia de 1.75, significa que al eliminar su relaci√≥n con el objetivo (mezclar sus valores aleatoriamente), el error del modelo aument√≥ en 1.75 unidades de RMSE.\n",
        "\n",
        "**¬øPor qu√© \"red\" est√° por encima de 1.75?**\n",
        "\n",
        "La variable \"red\" tiene un impacto fuerte en la predicci√≥n.\n",
        "Si la eliminamos o la permutamos, el error del modelo aumenta significativamente.\n",
        "Valores altos indican variables con mayor impacto en la predicci√≥n.\n",
        "Optimization History Plot (plot_optimization_history)\n",
        "üìâ Muestra c√≥mo el RMSE ha ido mejorando en cada prueba de Optuna.\n",
        "\n",
        "Importancia de los hiperpar√°metros (plot_param_importances)\n",
        "üìä Indica qu√© hiperpar√°metros tienen mayor impacto en el RMSE.\n",
        "\n",
        "Slice Plot (plot_slice)\n",
        "üìè Permite ver c√≥mo var√≠a el RMSE en funci√≥n de cada hiperpar√°metro.\n",
        "\n",
        "Parallel Coordinate Plot (plot_parallel_coordinate)\n",
        "üîó Muestra c√≥mo diferentes combinaciones de hiperpar√°metros afectan el RMSE.\n"
      ],
      "metadata": {
        "id": "DbqBQ6JDGZOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame con las coordenadas \"x\", \"y\", \"z\" y la profundidad predicha para cada conjunto\n",
        "\n",
        "# Agregar las predicciones al conjunto de datos original\n",
        "df_train = X_train.copy()\n",
        "df_train[\"x\"] = df_total.loc[X_train.index, \"x\"]\n",
        "df_train[\"y\"] = df_total.loc[X_train.index, \"y\"]\n",
        "df_train[\"z\"] = df_total.loc[X_train.index, \"z\"]\n",
        "df_train[\"Profundidad_Predicha\"] = y_train_pred_best_mlp\n",
        "\n",
        "df_test = X_test.copy()\n",
        "df_test[\"x\"] = df_total.loc[X_test.index, \"x\"]\n",
        "df_test[\"y\"] = df_total.loc[X_test.index, \"y\"]\n",
        "df_test[\"z\"] = df_total.loc[X_test.index, \"z\"]\n",
        "df_test[\"Profundidad_Predicha\"] = y_test_pred_best_mlp\n",
        "\n",
        "df_val = X_val.copy()\n",
        "df_val[\"x\"] = df_total.loc[X_val.index, \"x\"]\n",
        "df_val[\"y\"] = df_total.loc[X_val.index, \"y\"]\n",
        "df_val[\"z\"] = df_total.loc[X_val.index, \"z\"]\n",
        "df_val[\"Profundidad_Predicha\"] = y_val_pred_best_mlp\n",
        "\n",
        "# Seleccionar solo las columnas deseadas\n",
        "df_train = df_train[[\"x\", \"y\", \"z\", \"Profundidad_Predicha\"]]\n",
        "df_test = df_test[[\"x\", \"y\", \"z\", \"Profundidad_Predicha\"]]\n",
        "df_val = df_val[[\"x\", \"y\", \"z\", \"Profundidad_Predicha\"]]\n",
        "\n",
        "# Guardar en un archivo Excel con tres hojas\n",
        "output_file = \"Resultados_Predicciones.xlsx\"\n",
        "with pd.ExcelWriter(output_file) as writer:\n",
        "    df_train.to_excel(writer, sheet_name=\"Entrenamiento\", index=False)\n",
        "    df_test.to_excel(writer, sheet_name=\"Test\", index=False)\n",
        "    df_val.to_excel(writer, sheet_name=\"Validaci√≥n\", index=False)\n",
        "\n",
        "# Confirmar guardado\n",
        "print(f\"Archivo guardado: {output_file}\")\n"
      ],
      "metadata": {
        "id": "-YHBhIvaGaoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Al final del Notebook\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Tiempo total de ejecuci√≥n: {elapsed_time:.2f} segundos\")\n",
        "from google.colab import output\n",
        "\n",
        "# Al final del Notebook\n",
        "output.eval_js('alert(\"Ejecuci√≥n terminada üöÄ\")')"
      ],
      "metadata": {
        "id": "2WWSZqPn2e3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import smtplib\n",
        "\n",
        "def send_email():\n",
        "    sender_email = \"sgcortes@gmail.com\"  # Cambia esto por tu email\n",
        "    receiver_email = \"sgcortes@gmail.com\"  # Tambi√©n aqu√≠\n",
        "    password = \"tu_contrase√±a\"  # ‚ö†Ô∏è Usa una contrase√±a de aplicaci√≥n (NO la real)\n",
        "\n",
        "    message = \"Subject: Google Colab - Ejecuci√≥n terminada\\n\\nEl c√≥digo ha finalizado üöÄ\"\n",
        "\n",
        "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
        "        server.starttls()\n",
        "        server.login(sender_email, password)\n",
        "        server.sendmail(sender_email, receiver_email, message)\n",
        "\n",
        "send_email()"
      ],
      "metadata": {
        "id": "cFEf8XSm2umT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}