{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOLOry5CAY1TOAPZNNdKnJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgcortes/25_BathymetrySotoBarca/blob/main/v3_MLP_dataFromLocalFiles_03_2025_BATIMETRIA_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimación de la Batimetría a partir de imagen UAV multiespectral. MLP (Multilayer Perceptron)\n",
        "+Cota lámina de agua 212m\n",
        "\n",
        "---\n",
        "En la hoja excel adjunta (NorteEsteProf.xlsx) y en la en el hoja OesteProf.xlsx, se tienen datos de cotas del fondo (\"z\") de un embalse para puntos de posiciones (campos \"x\" e \"y\"). Además hay un conjunto de valores de niveles de reflectancia (campos: \"red\", \"green\", \"blue\", \"red_edge\", \"nir\") y de otros índices multiespectrales (\"ratiobg\", \"ndwi\", \"kwlightate\",\"ndvibarca\") que han sido calculados a partir de combinaciones de las variables de reflectancia.\n",
        "\n",
        "El objetivo es entrenar un modelo GradientBoost (sklearn) que relacione estas variables (excepto, \"fid\", \"x\", \"y\", \"z\") con la \"Profundidad\" (calculada como : 212-\"z\"). Los datos suministrados (alrededor de 16000 mas 3000) se dividirán en 60% para entrenamiento y 20% para test y 20% para validación.\n",
        "al final del entrenamiento dame las métricas del resultado obtenido para el conjunto test y el del entrenamiento y valiadción por separado. Además crea tres gráficas de valores predichos frente a valores reales para la profundad para el conjunto test y para el de entrenamiento y validación. Dibuja también una gráfica de importancia de ls variables en barras horizontales y ordeandas de mayor a menor."
      ],
      "metadata": {
        "id": "E9k20MB1YY28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura de datos"
      ],
      "metadata": {
        "id": "BG1MnbqlvPEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar Optuna si no está instalado (ya que el entorno fue reiniciado)\n",
        "!pip install optuna\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP8GJnzcX8vv",
        "outputId": "c918d9f4-0040-41b0-9aa3-c26528bdad46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Iniciar temporizador\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "liSIznp92P8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ACMnYrDEp-g9",
        "outputId": "c2e0c994-8ed0-4170-a958-9570cb805235"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'NorteEsteProf.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-042767a3345c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfile_oeste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"OesteProf.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_norte_este\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_norte_este\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf_oeste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_oeste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NorteEsteProf.xlsx'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "# Montar Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Definir la ruta del directorio en Google Drive\n",
        "#path = \"/content/drive/My Drive/Colab Notebooks/03_2025_BATIMETRIA-SOTO-BARCA/\"\n",
        "\n",
        "# Leer los archivos de Excel\n",
        "file_norte_este = \"NorteEsteProf.xlsx\"\n",
        "file_oeste = \"OesteProf.xlsx\"\n",
        "\n",
        "df_norte_este = pd.read_excel(file_norte_este)\n",
        "df_oeste = pd.read_excel(file_oeste)\n",
        "\n",
        "# Mostrar las primeras filas de los DataFrames para verificar la carga\n",
        "df_norte_este.head(), df_oeste.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recargar librerías necesarias después del reinicio del entorno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Definir las características y la variable objetivo\n",
        "features = [\"red\", \"green\", \"blue\", \"red_edge\", \"nir\", \"ratiobg\", \"ndwi\", \"kwlightate\", \"ndvibarca\"]\n",
        "target = \"Profundidad\"\n",
        "\n",
        "# Unir ambos conjuntos de datos en un solo DataFrame\n",
        "df_total = pd.concat([df_norte_este, df_oeste], ignore_index=True)\n",
        "\n",
        "# Imputar valores nulos con la mediana de cada columna\n",
        "df_total.fillna(df_total.median(), inplace=True)\n",
        "\n",
        "# Separar en variables independientes (X) y la variable objetivo (y)\n",
        "X = df_total[features]\n",
        "y = df_total[target]\n",
        "\n",
        "# Dividir en conjunto de entrenamiento (60%), test (20%) y validación (20%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Normalizar las características para mejorar la estabilidad del entrenamiento\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Definir la arquitectura de la red neuronal\n",
        "mlp_model = MLPRegressor(hidden_layer_sizes=(128, 64, 32),\n",
        "                         activation='relu',\n",
        "                         solver='adam',\n",
        "                         max_iter=1000,\n",
        "                         random_state=42)\n",
        "\n",
        "# Entrenar la red neuronal\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones con la red entrenada\n",
        "y_train_pred_mlp = mlp_model.predict(X_train_scaled)\n",
        "y_test_pred_mlp = mlp_model.predict(X_test_scaled)\n",
        "y_val_pred_mlp = mlp_model.predict(X_val_scaled)\n",
        "\n",
        "# Calcular métricas para el modelo MLP\n",
        "metrics_train_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_train, y_train_pred_mlp),\n",
        "    \"MSE\": mean_squared_error(y_train, y_train_pred_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_train, y_train_pred_mlp)),\n",
        "    \"R²\": r2_score(y_train, y_train_pred_mlp)\n",
        "}\n",
        "\n",
        "metrics_test_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_test, y_test_pred_mlp),\n",
        "    \"MSE\": mean_squared_error(y_test, y_test_pred_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_test_pred_mlp)),\n",
        "    \"R²\": r2_score(y_test, y_test_pred_mlp)\n",
        "}\n",
        "\n",
        "metrics_val_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_val, y_val_pred_mlp),\n",
        "    \"MSE\": mean_squared_error(y_val, y_val_pred_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_val, y_val_pred_mlp)),\n",
        "    \"R²\": r2_score(y_val, y_val_pred_mlp)\n",
        "}\n"
      ],
      "metadata": {
        "id": "NiNnWryM8zUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar métricas\n",
        "print(\"\\nTrain:\\n\",metrics_train_mlp, \"\\nTest:\\n\",metrics_test_mlp,\"\\nValidation:\\n\", metrics_val_mlp)\n"
      ],
      "metadata": {
        "id": "pQCwK6jO9l7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis:\n",
        "El modelo tiene un R² ≈ 0.74 en el conjunto de test, lo que indica que la red neuronal logra explicar una buena parte de la variabilidad de la profundidad.\n",
        "El rendimiento en test y validación es similar al de entrenamiento, lo que indica que el modelo no está sobreajustado.\n",
        "Sin embargo, en comparación con el Gradient Boosting, el desempeño parece un poco inferior."
      ],
      "metadata": {
        "id": "JY9_NfcJvTK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráficos predichos vs Reales"
      ],
      "metadata": {
        "id": "9YwDJloz9333"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear gráficos de valores predichos vs. valores reales\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Gráfico para el conjunto de entrenamiento\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_train, y_train_pred_mlp, alpha=0.5)\n",
        "plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Profundidad Real\")\n",
        "plt.ylabel(\"Profundidad Predicha\")\n",
        "plt.title(\"Entrenamiento: Valores Reales vs. Predichos\")\n",
        "\n",
        "# Gráfico para el conjunto de test\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(y_test, y_test_pred_mlp, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Profundidad Real\")\n",
        "plt.ylabel(\"Profundidad Predicha\")\n",
        "plt.title(\"Test: Valores Reales vs. Predichos\")\n",
        "\n",
        "# Gráfico para el conjunto de validación\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(y_val, y_val_pred_mlp, alpha=0.5)\n",
        "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Profundidad Real\")\n",
        "plt.ylabel(\"Profundidad Predicha\")\n",
        "plt.title(\"Validación: Valores Reales vs. Predichos\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5LWHo7qfrgJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis de los gráficos:\n",
        "\n",
        "En entrenamiento, los valores predichos siguen bastante bien la línea de identidad roja, lo que indica un buen ajuste.\n",
        "En test y validación, se mantiene una tendencia similar, aunque con mayor dispersión, lo que sugiere que la red neuronal generaliza razonablemente bien.\n"
      ],
      "metadata": {
        "id": "-_eeQ554-Agx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importancia de las variables"
      ],
      "metadata": {
        "id": "EZD77v6-vvmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Calcular la importancia de las variables usando Permutation Feature Importance\n",
        "perm_importance = permutation_importance(mlp_model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "# Convertir los resultados a un DataFrame\n",
        "importances_df = pd.DataFrame({'Variable': features, 'Importancia': perm_importance.importances_mean})\n",
        "importances_df = importances_df.sort_values(by='Importancia', ascending=True)\n",
        "\n",
        "# Graficar la importancia de las variables\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importances_df['Variable'], importances_df['Importancia'], color='skyblue')\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.ylabel(\"Variables\")\n",
        "plt.title(\"Importancia de las Variables en el Modelo MLP (Permutation Importance)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r3Sjn6ETv07G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Training:\\n\",metrics_train_mlp, \"\\nTest:\\n\",metrics_test_mlp,\"\\nValidation:\\n\", metrics_val_mlp)\n"
      ],
      "metadata": {
        "id": "4grzz-I6wZOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis:\n",
        "\n",
        "Algunas variables tienen una mayor influencia en la predicción de la profundidad del embalse.\n",
        "Al no haber una estructura clara como en los modelos de árboles, la influencia de cada variable puede ser más distribuida y dependiente de interacciones no lineales.\n",
        "Las variables con mayor importancia indican que tienen una mayor contribución a la precisión del modelo."
      ],
      "metadata": {
        "id": "XZYBdGLawjJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guardado del mejor modelo"
      ],
      "metadata": {
        "id": "pNLsUpIpyZbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar el modelo en un archivo\n",
        "joblib.dump(mlp_model, \"mlp_regressor_model.pkl\")\n",
        "print(\"Modelo MLP guardado como 'mlp_regressor_model.pkl'\")\n"
      ],
      "metadata": {
        "id": "Rqn6AGNjycXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga del mejor modelo y prediccion"
      ],
      "metadata": {
        "id": "Kd1xVMvpyhH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo desde el archivo\n",
        "loaded_mlp_model = joblib.load(\"mlp_regressor_model.pkl\")\n",
        "\n",
        "# Usar el modelo cargado para hacer predicciones\n",
        "y_pred_loaded = loaded_mlp_model.predict(X_test_scaled)\n",
        "\n",
        "# Mostrar las primeras 5 predicciones\n",
        "print(\"Predicciones con el modelo cargado:\", y_pred_loaded[:5])"
      ],
      "metadata": {
        "id": "BWmPBrhtymVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conjunto de datos"
      ],
      "metadata": {
        "id": "ECgZ-eTT3-Yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El grupo de datos completamente independiente, que no ha intervenido en ningún momento en el entrenamiento ni en la optimización de hiperparámetros, es el conjunto de **test**.\n",
        "\n",
        "**Entrenamiento (60%):**\n",
        "\n",
        "Se usó para ajustar el modelo tanto en la versión inicial como en la versión optimizada con Optuna.\n",
        "Participó en la estimación de los mejores valores de los hiperparámetros.\n",
        "\n",
        "**Validación (20%):**\n",
        "\n",
        "Se usó en Optuna para evaluar los diferentes modelos mientras se ajustaban los hiperparámetros.\n",
        "Se utilizó en el cálculo del RMSE para determinar el mejor conjunto de hiperparámetros.\n",
        "\n",
        "**Test (20%):**\n",
        "\n",
        "No se usó en ningún momento durante el entrenamiento ni en la optimización de hiperparámetros.\n",
        "Se utilizó únicamente después de obtener el mejor modelo para evaluar su capacidad de generalización en datos completamente nuevos."
      ],
      "metadata": {
        "id": "gVFNgy5O6x7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MEJORA DE LA RED NEURONAL\n",
        "### Optimización con Optuna\n",
        "¿Qué vamos a optimizar?\n",
        "1. Número de capas ocultas (entre 1 y 4 capas).\n",
        "2. Número de neuronas por capa (entre 32 y 512).\n",
        "3. Tasa de regularización alpha (para evitar sobreajuste).\n",
        "4. Tipo de optimizador (adam vs sgd).\n",
        "5. Número de iteraciones (max_iter)."
      ],
      "metadata": {
        "id": "fn92x-qh_WL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar Optuna\n",
        "import optuna\n",
        "import optuna.visualization as vis\n",
        "# Definir la función de optimización con Optuna\n",
        "def objective(trial):\n",
        "    # Hiperparámetros a optimizar\n",
        "    hidden_layer_1 = trial.suggest_int(\"hidden_layer_1\", 32, 512)\n",
        "    hidden_layer_2 = trial.suggest_int(\"hidden_layer_2\", 32, 512)\n",
        "    hidden_layer_3 = trial.suggest_int(\"hidden_layer_3\", 32, 512)\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-1,log=True)  # Regularización L2\n",
        "    solver = trial.suggest_categorical(\"solver\", [\"adam\", \"sgd\"])\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 500, 2000)\n",
        "\n",
        "    # Definir el modelo con los hiperparámetros seleccionados\n",
        "    mlp_model = MLPRegressor(hidden_layer_sizes=(hidden_layer_1, hidden_layer_2, hidden_layer_3),\n",
        "                             activation='relu',\n",
        "                             solver=solver,\n",
        "                             alpha=alpha,\n",
        "                             max_iter=max_iter,\n",
        "                             random_state=42)\n",
        "\n",
        "    # Entrenar la red neuronal\n",
        "    mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluar en el conjunto de validación\n",
        "    y_val_pred = mlp_model.predict(X_val_scaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))  # Minimizar RMSE\n",
        "\n",
        "    return rmse\n",
        "\n",
        "# Ejecutar la optimización con Optuna\n",
        "study = optuna.create_study(direction=\"minimize\")  # Minimizar el RMSE\n",
        "study.optimize(objective, n_trials=25)  # Ejecutar 30 pruebas de hiperparámetros\n",
        "\n",
        "# Obtener los mejores hiperparámetros encontrados\n",
        "best_params_mlp = study.best_params\n",
        "\n",
        "# 📊 Gráficos de análisis de Optuna\n",
        "# 1️⃣ Optimization History Plot\n",
        "fig1 = vis.plot_optimization_history(study)\n",
        "fig1.show()\n",
        "\n",
        "# 2️⃣ Importancia de los hiperparámetros\n",
        "fig2 = vis.plot_param_importances(study)\n",
        "fig2.show()\n",
        "\n",
        "# 3️⃣ Slice Plot (impacto de cada hiperparámetro en RMSE)\n",
        "fig3 = vis.plot_slice(study)\n",
        "fig3.show()\n",
        "\n",
        "# 4️⃣ Parallel Coordinate Plot (relación entre hiperparámetros y RMSE)\n",
        "fig4 = vis.plot_parallel_coordinate(study)\n",
        "fig4.show()"
      ],
      "metadata": {
        "id": "F8_VuQ965HY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el mejor modelo con los hiperparámetros óptimos\n",
        "#best_mlp_model = MLPRegressor(**best_params_mlp, activation='relu', random_state=42)\n",
        "hidden_layers = (best_params_mlp[\"hidden_layer_1\"],\n",
        "                 best_params_mlp[\"hidden_layer_2\"],\n",
        "                 best_params_mlp[\"hidden_layer_3\"])\n",
        "\n",
        "# Crear el modelo MLP con los mejores hiperparámetros\n",
        "best_mlp_model = MLPRegressor(hidden_layer_sizes=hidden_layers,\n",
        "                              activation='relu',\n",
        "                              solver=best_params_mlp[\"solver\"],\n",
        "                              alpha=best_params_mlp[\"alpha\"],\n",
        "                              max_iter=best_params_mlp[\"max_iter\"],\n",
        "                              random_state=42)\n",
        "\n",
        "best_mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones con el mejor modelo optimizado\n",
        "y_train_pred_best_mlp = best_mlp_model.predict(X_train_scaled)\n",
        "y_test_pred_best_mlp = best_mlp_model.predict(X_test_scaled)\n",
        "y_val_pred_best_mlp = best_mlp_model.predict(X_val_scaled)\n",
        "\n",
        "# Calcular métricas del modelo optimizado\n",
        "metrics_train_best_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_train, y_train_pred_best_mlp),\n",
        "    \"MSE\": mean_squared_error(y_train, y_train_pred_best_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_train, y_train_pred_best_mlp)),\n",
        "    \"R²\": r2_score(y_train, y_train_pred_best_mlp)\n",
        "}\n",
        "\n",
        "metrics_test_best_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_test, y_test_pred_best_mlp),\n",
        "    \"MSE\": mean_squared_error(y_test, y_test_pred_best_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_test_pred_best_mlp)),\n",
        "    \"R²\": r2_score(y_test, y_test_pred_best_mlp)\n",
        "}\n",
        "\n",
        "metrics_val_best_mlp = {\n",
        "    \"MAE\": mean_absolute_error(y_val, y_val_pred_best_mlp),\n",
        "    \"MSE\": mean_squared_error(y_val, y_val_pred_best_mlp),\n",
        "    \"RMSE\": np.sqrt(mean_squared_error(y_val, y_val_pred_best_mlp)),\n",
        "    \"R²\": r2_score(y_val, y_val_pred_best_mlp)\n",
        "}\n",
        "\n",
        "# Mostrar los mejores hiperparámetros y las métricas optimizadas\n",
        "print(\"Best Hyperpars:\\n\",best_params_mlp,\"\\nBest MLP Train:\\n\", metrics_train_best_mlp,\"\\nTest MLP Test:\\n\", metrics_test_best_mlp, \"\\nBest MLP Val:\\n\",metrics_val_best_mlp)\n"
      ],
      "metadata": {
        "id": "dFz8WN-uXzK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En las redes neuronales MLP (Multilayer Perceptron) no hay una métrica directa de importancia de las variables.\n",
        "\n",
        "Para estimar la importancia de las variables en un MLP, usamos **Permutation Feature Importance**, que es una técnica basada en el impacto de cada variable en la predicción.\n",
        "Proceso de cálculo:\n",
        "\n",
        "Se calcula el RMSE original del modelo.\n",
        "Se mezcla aleatoriamente una de las variables de entrada.\n",
        "Se vuelve a calcular el RMSE del modelo con la variable alterada.\n",
        "La importancia de la variable se define como la diferencia entre el RMSE original y el RMSE después de la permutación.\n",
        "Cuanto más aumente el error al desordenar una variable, más importante es esa variable.\n",
        "\n",
        "**¿Las importancias son relativas?**\n",
        " No son relativas ni normalizadas.\n",
        " Son valores absolutos de impacto en el error del modelo.\n",
        "\n",
        "Si una variable tiene una importancia de 1.75, significa que al eliminar su relación con el objetivo (mezclar sus valores aleatoriamente), el error del modelo aumentó en 1.75 unidades de RMSE.\n",
        "\n",
        "**¿Por qué \"red\" está por encima de 1.75?**\n",
        "\n",
        "La variable \"red\" tiene un impacto fuerte en la predicción.\n",
        "Si la eliminamos o la permutamos, el error del modelo aumenta significativamente.\n",
        "Valores altos indican variables con mayor impacto en la predicción.\n",
        "Optimization History Plot (plot_optimization_history)\n",
        "📉 Muestra cómo el RMSE ha ido mejorando en cada prueba de Optuna.\n",
        "\n",
        "Importancia de los hiperparámetros (plot_param_importances)\n",
        "📊 Indica qué hiperparámetros tienen mayor impacto en el RMSE.\n",
        "\n",
        "Slice Plot (plot_slice)\n",
        "📏 Permite ver cómo varía el RMSE en función de cada hiperparámetro.\n",
        "\n",
        "Parallel Coordinate Plot (plot_parallel_coordinate)\n",
        "🔗 Muestra cómo diferentes combinaciones de hiperparámetros afectan el RMSE.\n"
      ],
      "metadata": {
        "id": "DbqBQ6JDGZOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame con las coordenadas \"x\", \"y\", \"z\" y la profundidad predicha para cada conjunto\n",
        "\n",
        "# Agregar las predicciones al conjunto de datos original\n",
        "df_train = X_train.copy()\n",
        "df_train[\"x\"] = df_total.loc[X_train.index, \"x\"]\n",
        "df_train[\"y\"] = df_total.loc[X_train.index, \"y\"]\n",
        "df_train[\"z\"] = df_total.loc[X_train.index, \"z\"]\n",
        "df_train[\"Profundidad_Predicha\"] = y_train_pred_best_mlp\n",
        "\n",
        "df_test = X_test.copy()\n",
        "df_test[\"x\"] = df_total.loc[X_test.index, \"x\"]\n",
        "df_test[\"y\"] = df_total.loc[X_test.index, \"y\"]\n",
        "df_test[\"z\"] = df_total.loc[X_test.index, \"z\"]\n",
        "df_test[\"Profundidad_Predicha\"] = y_test_pred_best_mlp\n",
        "\n",
        "df_val = X_val.copy()\n",
        "df_val[\"x\"] = df_total.loc[X_val.index, \"x\"]\n",
        "df_val[\"y\"] = df_total.loc[X_val.index, \"y\"]\n",
        "df_val[\"z\"] = df_total.loc[X_val.index, \"z\"]\n",
        "df_val[\"Profundidad_Predicha\"] = y_val_pred_best_mlp\n",
        "\n",
        "# Seleccionar solo las columnas deseadas\n",
        "df_train = df_train[[\"x\", \"y\", \"z\", \"Profundidad_Predicha\"]]\n",
        "df_test = df_test[[\"x\", \"y\", \"z\", \"Profundidad_Predicha\"]]\n",
        "df_val = df_val[[\"x\", \"y\", \"z\", \"Profundidad_Predicha\"]]\n",
        "\n",
        "# Guardar en un archivo Excel con tres hojas\n",
        "output_file = \"Resultados_Predicciones.xlsx\"\n",
        "with pd.ExcelWriter(output_file) as writer:\n",
        "    df_train.to_excel(writer, sheet_name=\"Entrenamiento\", index=False)\n",
        "    df_test.to_excel(writer, sheet_name=\"Test\", index=False)\n",
        "    df_val.to_excel(writer, sheet_name=\"Validación\", index=False)\n",
        "\n",
        "# Confirmar guardado\n",
        "print(f\"Archivo guardado: {output_file}\")\n"
      ],
      "metadata": {
        "id": "-YHBhIvaGaoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Al final del Notebook\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Tiempo total de ejecución: {elapsed_time:.2f} segundos\")\n",
        "from google.colab import output\n",
        "\n",
        "# Al final del Notebook\n",
        "output.eval_js('alert(\"Ejecución terminada 🚀\")')"
      ],
      "metadata": {
        "id": "2WWSZqPn2e3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import smtplib\n",
        "\n",
        "def send_email():\n",
        "    sender_email = \"sgcortes@gmail.com\"  # Cambia esto por tu email\n",
        "    receiver_email = \"sgcortes@gmail.com\"  # También aquí\n",
        "    password = \"tu_contraseña\"  # ⚠️ Usa una contraseña de aplicación (NO la real)\n",
        "\n",
        "    message = \"Subject: Google Colab - Ejecución terminada\\n\\nEl código ha finalizado 🚀\"\n",
        "\n",
        "    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
        "        server.starttls()\n",
        "        server.login(sender_email, password)\n",
        "        server.sendmail(sender_email, receiver_email, message)\n",
        "\n",
        "send_email()"
      ],
      "metadata": {
        "id": "cFEf8XSm2umT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}